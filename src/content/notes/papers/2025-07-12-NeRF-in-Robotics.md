---
title: NeRF in Robotics
tags: [paper,VLA]
sidebar:
  nav: cs
mathjax: true
key: 2025-07-12-NeRF-in-Robotics
aside:
    toc: true
---
普通的论文阅读笔记

<!--more-->

## 引言

神经渲染（Neural rendering）是一类结合机器学习和计算机图形学物理模型来生成图像或视频的方法。它能够在生成逼真视图的同时，允许对场景属性进行显式或隐式控制。神经辐射场（NeRF）训练一个神经网络，其参数编码了场景的特定隐式表示。NeRF中的体积渲染，为核心组件，使NeRF能够从一组已知相机姿态的2D图像中学习连续的3D场景表示，并支持从任意视角进行新视图渲染。
![20250712134621](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250712134621.png)

## 背景

![20250712135130](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250712135130.png)

NeRF将场景建模为一个5D向量值函数，该函数由一个多层感知机（MLP）$F_\Theta:(\mathbf{x},\mathbf{d})\to(\mathbf{c},\sigma)$近似表示。（注释：MLP理论上可以逼近任意复杂的连续函数，NeRF的目标是构建一个从5D坐标$(\mathbf{x},\mathbf{d})$到颜色$\mathbf{c}$和密度$\sigma$的映射函数，这是一个高度非线性的函数，故选用MLP。$\mathbf{x}$是三维空间位置，$\mathbf{d}$是观察方向）网络输出一个RGB颜色向量$\mathbf{c}=(r,g,b)$和一个体密度$\sigma$。体密度描述空间中某点挡住光线的能力（如实体，半透明，真空），颜色体现物体材质。NeRF通过体渲染生成目标图像。整个网络通过比较渲染出的图像和真实观测图像来优化学习权重进行训练。 <br/>
NeRF的训练过程如上图所示，分为四个部分： <br/>
(a) NeRF假设一组光线从相机中心发出，穿过图像中的每个像素进入场景。沿着每条射线采样一组点。这些采样点的5D坐标（3D位置+2D方向）通过位置编码后输入多层感知机。在位置编码中，一组基函数将坐标映射到更高维的空间，使MLP能够捕获高频空间信息并更好地表示细粒度的场景特征。 <br/>
(b) 网络输出采样点的体密度$\sigma$和颜色$\mathbf{c}$。体密度$\sigma$仅与位置相关，而颜色$\mathbf{c}$则与位置和观察方向都相关。 <br/>
(c) 体渲染通过沿相应射线积分采样点的密度加权颜色，计算出目标像素的颜色。 <br/>
(d) 渲染损失通常定义为每个目标像素的预测颜色与真实颜色之间的平方误差，通过最小化该损失来优化网络参数。 <br/>
具体而言，体渲染沿着每条射线执行积分，通过累积所有采样点的颜色贡献（按其密度和可见性加权），来计算沿观察方向 $\mathbf{d}$的目标图像中的最终像素值：

$$
C(\mathbf{r})=\int_{t_n}^{t_f}\underbrace{T(t)}_{\text{累积透射率}}\cdot\underbrace{\sigma(\mathbf{r}(t))}_{\text{密度}}\cdot\underbrace{\mathbf{c}(\mathbf{r}(t),\mathbf{d})}_{\text{颜色}}dt
$$ 

其中$\mathbf{t_n}$和$\mathbf{t_f}$是相机光线$\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$的近边界和远边界。$\mathbf{T(t)}$被计算为光线从$\mathbf{t_n}$传播到$\mathbf{t}$的透射率： <br/>
$T(t)=\exp\left(-\int_{t_n}^t\sigma(\mathbf{r}(s))ds\right)$ <br/>
由于点采样的离散性质，NeRF使用以下离散公式来近似理想的连续体积积分： <br/>
$\hat{C}(\mathbf{r})=\sum_{i=1}^NT_i\alpha_i\mathbf{c}_i,\quad\text{其中}\quad T_i=\exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right)$ <br/>
$\alpha_i=(1-\exp(-\sigma_i\delta_i))$，$\delta_i=t_{i+1}-t_i$是相邻采样点之间的距离。 <br/>
基于此设计，NeRF还融入了两项额外技术：位置编码以提升表示质量，以及分层体采样以提高计算效率。 <br/>
位置编码定义为$F_{\Theta}=F_{\Theta}^{\prime}\circ\gamma$，它使用$\gamma(p)$将输入向量映射到高维空间，以便更好地表示场景颜色和几何的高频变化：

$$
\begin{aligned}\boldsymbol{\gamma}(p)=&(\sin(2^0\pi p),\cos(2^0\pi p),...,\sin(2^{L-1}\pi p),\cos(2^{L-1}\pi p))\end{aligned}
$$

，其中${L}$是一个超参数。在NeRF中，对$\gamma(\mathbf{x})$使用${L=10}$，对$\gamma(\mathbf{d})$使用${L=4}$。$\gamma(\mathbf{x})$在MLP开始时注入网络，而$\gamma(\mathbf{d})$在接近末端时注入，该方法可以缓解退化解问题。 <br/>
分层次渲染采用由粗到精的策略：首先粗略采样${N_c}$个点以生成初始渲染结果。然后这个粗略结果指导精细采样，选择${N_f}$个精细层级的点。其目的是将采样集中在那些对最终像素颜色贡献更显著的区域。 <br/>
最后，分层体渲染的损失函数定义如下： <br/>
$L=\sum_{\mathbf{r}\in R}\left[\left\|\hat{C}_c(\mathbf{r})-C(\mathbf{r})\right\|_2^2+\left\|\hat{C}_f(\mathbf{r})-C(\mathbf{r})\right\|_2^2\right]$ <br/>
其中${R}$是射线集合，${C(r)}$是真实颜色，$\hat{C_c}{(r)}$和$\hat{C_f}{(r)}$分别是粗略网络和精细网络预测的颜色。

## 应用

![20250714173106](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250714173106.png)

### Static Reconstruction

![20250714182034](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250714182034.png)

iMAP结合了MLP（多层感知机）架构和一种体积密度表示，该方法受到了NeRF的启发，用于SLAM任务。iMAP仅用2D图像作为输入，利用损失引导的采样和重放缓冲机制，但由于MLP结构容量有限，导致出现灾难性遗忘和推理速度慢等问题。此外，体积密度是一种概率表示，存在外观-几何模糊性，可能导致低精度重建。 <br/>
为了扩展重建规模，MeSLAM采用多MLP结构来表示场景的不同部分，而NICESLAM引入了从粗到细的特征网格表示，将iMAP的能力从单房间扩展到多房间重建。Vox-Fusion使用类树结构来存储网格嵌入，可以在场景扩展时动态分配新的空间体素；Lisus和Holmes表明，融合深度不确定性和运动信息可以提高SLAM精度，还可以采用球形背景模型来扩展重建场景的尺度，为提高重建效率，ESLAM用多尺度轴上对齐的垂直平面代替特征网格，将场景规模增长从立方级降低到平方级。 <br/>
在使用室外数据集的研究中， Neural 3d reconstruction in the wild.中采用外观嵌入（如NeRF-W，Martin-Brualla等）来建模外观变化，并提出体素引导采样与表面引导采样相结合以提高大规模场景的效率。Block-NeRF将大规模场景划分为多个空间分块来建模包含复杂交叉口的长街道，各分块对对新视角渲染的贡献由学习得到的可见性权重调节。NeRFLOMA是专为户外驾驶环境设计的NeRF基于纯LiDAR的SLAM系统。它采用神经符号距离函数，优化神经隐式解码器，将八叉树网格中的嵌入解码为SDF值。通过最小化SDF误差，NeRFLOMA同时优化嵌入、位姿和解码器，最终实现稠密平滑的网格地图重建。类似地，LidaRF使用3D稀疏卷积从点云中提取几何特征，构建基于网格的表示，并通过LiDAR投影生成增强训练数据，在鲁棒的深度监督方案下进行几何预测训练。GeoNLF是一个混合框架，在全局神经重建与纯几何位姿优化之间交替运行。它通过点云的丰富几何特征，在帧间点云对应关系上加入额外的Chamfer损失，以补充标准的BA优化与光度监督，从而联合优化相机位姿并提高地图质量。 <br/>
近期，基于NeRF架构的截断符号距离函数与主动场景重建技术取得了显著进展。与普通NeRF的体积密度表示不同，TSDF编码的是从采样点到最近表面的距离，因此可以更明确地进行几何重建。<br/>
主动场景重建技术旨在探索如何使机器人主动选择最有利于重建的数据，提升智能化重建过程的效率。NeurAR将像素颜色建模为高斯分布的随机变量，明确表示观测不确定性，并将其与峰值信噪比相关联，用于度量候选视角的质量。AutoNeRF通过模块化策略探索，学习机器人自主数据采集策略，并将场景语义作为评估标准。

#### 找c哥补全的一些知识

* 1.为什么体积密度表示几何是概率性的 <br/>
NeRF表示每个3D空间点的密度和颜色，描述了光线在穿过该点时被阻挡的概率。利用的是体积渲染方程，其核心思想是：当一条光线穿过空间，它会被不同的点以不同程度地吸收或反射，NeRF估计这条光线最终的颜色是这些点的加权平均。 <br/>
想象你拿一根激光笔往雾里照，每一点的“雾浓度”决定了激光是否被阻挡。NeRF没有直接告诉你“墙在这儿”，而是告诉你“从这里开始，空气越来越浓了”，你只能推断表面可能在密度迅速上升的位置。这就是“概率性”：你不是得到确定的几何边界，而是通过密度分布猜测哪里可能是物体表面。

* 2.什么是经验回放和损失引导采样 <br/>
经验回放即系统保存之前看过的训练样本，新的训练批次不仅包括新帧的数据，还从记忆里随机调一些老数据一起训练，从而避免灾难性遗忘。 <br/>
NeRF训练是通过从图像中发射射线，然后在空间中采样很多点计算损失。但这些点一般是均匀采样的，效率不高。损失引导采样会统计哪些空间区域产生的重建误差大，在这些区域采样更多点以提高训练精度，同时忽略那些已经重建得很好的区域。

* 为什么用指数函数建模吸收概率
光线在一段路中不被吸收的概率满足泊松分布

* SDF、TSDF与体积密度的区别（几何表示的三种范式）
在NeRF系统中有三种主流方式来表示场景中的几何结构： <br/>
1. 体积密度
不存储表面在哪，而是给每个空间点一个非负实数，表示该点能否阻挡光线，然后用体积渲染公式估算最终颜色。 <br/>
缺点：
    - 没有明确的表面定义
    - 外观和几何是耦合的
    - 灾难性遗忘严重
2. 有符号距离函数SDF (Signed Distance Function)
![20250716141954](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250716141954.png) <br/>
SDF表示每个点x到最近表面的距离，并加上符号：
$$
\mathrm{SDF}(\mathbf{x}) = \begin{cases} -d & \text{如果点在物体内部} \\ 0 & \text{如果点在物体表面} \\ +d & \text{如果点在物体外部} \end{cases}
$$ 
3. 截断的SDF（TSDF, Truncated Signed Distance Function） <br/>
TSDF是对SDF的一个实际改进版本，目的是节省存储并增强数值稳定性。 <br/>
定义如下：
$\mathrm{TSDF}(\mathbf{x})=\mathrm{clip}(\mathrm{SDF}(\mathbf{x}),-\tau,+\tau)$ <br/>
即，超过阈值$\tau$的部分都被截断为$\pm\tau$
![20250716143738](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250716143738.png) <br/>

### Dynamic Reconstruction

长期运行的机器人通常会面临复杂环境中的动态变化。传统的NeRF模型基于静态场景假设，因此动态变化无疑会干扰其学习过程，导致图像伪影。此外，在动态场景中，每个时刻只能提供一次观察，这会导致视角之间缺乏空间一致性约束。因此，在动态环境中，NeRF模型必须被扩展或以不同的方式进行训练。 <br/>
![20250716151040](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250716151040.png) <br/>
在早期的探索中，研究者通过将NeRF与额外输入（如时间或相机位姿变换）进行条件耦合，端到端地建模场景动态性。STaR提出了一种刚性动态NeRF，用于表示场景中的单个运动物体，并通过优化与时间相关的刚性姿态来追踪其运动。DyNeRF使用时间相关的潜编码而非直接输入时间变量，用于建模动态场。这种方式能更好地表达拓扑结构的变化及瞬态现象。

#### 🧩 基于形变场的动态 NeRF 方法（Deformation-based）

| 模型名称 | 核心方法 / 思路 | 为什么这么做 | 带来的好处 |
|----------|------------------|----------------|----------------|
| **D-NeRF** (Pumarola et al., 2021) | 时间条件 + 从观测空间变换至 canonical 空间 | 显式建模随时间变化的形变轨迹 | 表达非刚体运动，重建连续变形物体 |
| **NR-NeRF** (Tretschk et al., 2021) | 不预设 canonical 空间，联合学习；隐式时间编码 | 避免初始帧偏差，提高时间建模能力 | 提升渲染质量，更灵活应对动态变化 |
| **NeRFies** (Park et al., 2021a) | 使用密集 SE(3) 场代替位移场；加入弹性能量约束 | 精细建模局部旋转与拉伸，防止解模糊 | 优化收敛更稳定，表达更物理合理 |
| **HyperNeRF** (Park et al., 2021b) | 引入高维超空间，拓扑变化视为超空间切片 | 处理拓扑结构变化（如打开的门） | 支持建模拓扑变形场景，表现更自然 |
| **NeRF-DS** (Yan et al., 2023) | 条件化颜色分支：表面位置 + 法向量旋转 | 解决镜面反射下欠参数问题 | 提升动态反射物体的细节和真实性 |
| **D2NeRF** (Wu et al., 2022) | 增加阴影场学习动态阴影比例 | 光照和阴影变化影响真实感 | 更真实的渲染阴影变化，增强动态感 |
| **RoDynRF** (Liu et al., 2023a) | 联合优化形变场 + 相机位姿 + 焦距 | COLMAP 不适用于复杂动态场景 | 提升动态场景下的相机跟踪与重建精度 |
| **TiNeuVox** (Fang et al., 2022) | 显式时间敏感神经体素结构 | 替代慢速 MLP 查询，提升效率 | 训练和渲染速度快，适合实时应用 |

#### 🌊 基于流场的动态 NeRF 方法（Flow-based）

| 模型名称 | 核心方法 / 思路 | 为什么这么做 | 带来的好处 |
|----------|------------------|----------------|----------------|
| **NSFF** (Li et al., 2021b) | 同时预测双向场景流 + 遮挡权重 | 显式建模场景整体运动 | 提升动态区域一致性与配准精度 |
| **Dynamic view synthesis from dynamic monocular video** | 分为静态/动态 NeRF + 场景流混合权重 | 分离建模 + 动态遮罩辅助 | 适应多对象动态场景，优化稳定性 |
| **EmerNeRF** (Yang et al., 2023) | 自监督分离静/动态成分，聚合时间特征 | 避免标注依赖，增强跨帧一致性 | 可用于弱监督或无监督学习的动态场景 |
| **SUDS** (Turki et al., 2023) | 静态+动态+远场 NeRF，动态部分估计场景流 | 适配大规模城市街景类数据 | 精细表达不同距离尺度下的动态性 |
| **Decoupling dynamic monocular videos for dynamic view synthesis, 2024** | 表面一致性 + 多视图 patch 约束代替光流 | 去除对光流网络依赖 | 完全无监督，增强泛化能力 |
| **FlowIBR** (Büsching et al., 2024) | 静态预训练 + 场景特定流场联合推理 | 结合静态知识迁移到动态场景 | 融合静态优势和动态补偿，泛化能力强 |
| **Du et al., 2021** | 预测速度流，积分获得未来位置 | 直接预测物体轨迹，适用于未来帧预测 | 拓展NeRF到时序预测领域 |

#### 🧠 多平面动态建模方法

| 模型名称 | 核心方法 / 思路 | 为什么这么做 | 带来的好处 |
|----------|------------------|----------------|----------------|
| **K-Planes** (Fridovich-Keil et al., 2023) | 六个可学习的时空特征平面表示 | 替代体素或 MLP，压缩内存 | 渲染高质量同时节省资源 |
| **HexPlane** (Cao & Johnson, 2023) | 六边形映射构造特征平面结构 | 增强特征排列结构性 | 更紧凑，进一步优化内存与训练效率 |

### 3D场景分割

![20250717185556](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250717185556.png)

#### 语义分割

| 模型名称 | 核心方法 / 思路 | 为什么这么做 | 带来的好处 |
|----------|------------------|----------------|----------------|
| **Semantic-NeRF** (Zhi et al., 2021a) | 在NeRF中加入语义头，用于采样点的语义预测 | 直接从采样点估计语义标签 | 实现端到端的三维语义标注 |
| **NeSF** (Vora et al., 2022) | 多场景共享的3D UNet + 语义MLP解码NeRF密度场 | 学习可泛化的语义表示 | 支持跨场景的语义泛化 |
| **iLabel** (Zhi et al., 2021b) | 使用稀疏语义标签实现在线交互式语义SLAM | 降低对精确语义标注的依赖 | 实时语义SLAM，降低人工成本 |
| **Baking in the feature: Accelerating volumetric segmentation by rendering feature maps(Blomqvist et al., 2023)** | 使用大数据集预训练的特征提取器烘焙特征 | 弥补稀疏标签带来的语义缺失 | 提升上游语义特征质量 |
| **Unsupervised continual semantic adaptation through neural rendering(Liu et al., 2023b)** | 自监督训练统一的分割模型 + 多个scene-specific Semantic-NeRF | 利用伪标签迭代优化语义一致性 | 自监督迭代精炼，提升分割精度 |
| **SNI-SLAM** (Zhu et al., 2024) | 多模态特征交互（颜色+几何+语义） | 利用跨模态特征互补性 | 实现更精准的渲染与语义联合建模 |
| **GOV-NeSF** (Wang et al., 2024) | 使用LSeg提取语义特征 + 多视角融合与交叉注意力 | 利用2D模型泛化能力提升3D语义 | 无需3D标注，实现弱监督语义分割 |

#### 实例分割

| 模型名称 | 核心方法 / 思路 | 为什么这么做 | 带来的好处 |
|----------|------------------|----------------|----------------|
| **uORF** (Yu et al., 2022a) | 使用图像中对象中心潜编码条件化NeRF训练 | 从单图像推理出实例表示 | 可控渲染，支持实例级编辑 |
| **ONeRF** (Liang et al., 2022) | 利用特征聚类 + 3D一致性进行无监督分割 | 在无标签条件下提取目标实例 | 自动生成3D掩码，支持对象提取 |
| **N3F** (Tschernezki et al., 2022) | 教师-学生框架，结合CLIP/LSeg等2D特征 | 用2D大模型蒸馏3D实例信息 | 精准蒸馏，降低训练标注成本 |
| **SA3D** (Cen et al., 2023) | SAM分割+NeRF 3D掩膜传播+跨视角引导 | 高质量掩膜跨视角一致传播 | 精细目标建模，提升3D分割完整性 |
| **NeuralDiff** (Tschernezki et al., 2021) | 三分支网络分别建模前景、背景和演员 | 区分视频中的活动主体 | 实现**第一人称视频**中的对象分离 |

#### 全景分割

| 模型名称 | 核心方法 / 思路 | 为什么这么做 | 带来的好处 |
|----------|------------------|----------------|----------------|
| **Panoptic-NeRF** (Fu et al., 2022) | 构建固定/可学习语义场 + 3D框辅助监督 | 解决伪标签噪声，补充实例信息 | 同时提升几何/语义质量，适合自动驾驶场景 |
| **PNF** (Kundu et al., 2022) | 每个前景对象用独立轻量MLP建模 | 替代共享网络，消除实例干扰 | 精确追踪单个对象，支持姿态估计 |
| **Panoptic Lifting** (Siddiqui et al., 2023) | 使用2D全景掩码构建一致的3D表示 | 仅用2D模型实现高效3D合成 | 统一多视角语义+实例，全景一致性好 |

### 场景编辑

![20250717185921](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250717185921.png)
![20250717185930](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20250717185930.png)

#### 外观与几何编辑


| 方法 | 核心思路 | 为什么这么做 | 带来的好处 |
|------|-----------|---------------|--------------|
| **CodeNeRF** (Jang & Agapito, 2021) | 解耦 shape 与 appearance 编码，控制 NeRF 生成 | 避免形状/材质相互干扰 | 可独立修改形状或外观 |
| **EditNeRF** (Liu et al., 2021) | 在调整编码的同时更新部分网络层权重 | 提升编辑灵活度与局部控制能力 | 支持更精细的风格/结构调整 |
| **CLIP-NeRF** (Wang et al., 2022a) | 利用 CLIP 多模态模型，引导NeRF按文本或图像生成 | 融合语言/视觉指导进行编辑 | 支持“以图/文生图”式修改 |
| **SINE** (Bao et al., 2023) | 引入先验引导的编辑场，调整坐标和颜色 | 实现语义驱动的目标区域编辑 | 编辑效果更贴合语义目标 |
| **PartNeRF** (Tertikas et al., 2023) | 每个物体部分对应独立 NeRF，以局部编码控制 | 支持局部区域修改而不影响整体 | 精细化局部编辑，多级可控 |
| **Deforming radiance fields with cages（Xu & Harada，2022）** | 通过控制封装 cage 网格顶点实现变形 | 显式操作简化精确几何修改 | 提高几何编辑精度与可解释性 |
| **CageNeRF** (Peng et al., 2022) | 学习从 cage + pose → 变形 cage 的映射网络 | 网络自动处理 mesh 变形过程 | 用户友好，自动化几何变换 |
| **NeRF-Editing** (Yuan et al., 2022b) | 用经典 mesh 变形算法编辑 canonical mesh，再变换回 NeRF | 结合显式控制与隐式表达优势 | 支持直接操控 mesh 实现重建 |
| **NeuMesh** (Yang et al., 2022a) | 在 mesh 顶点中存储 learnable 编码，几何/外观解码器重建 | 明确结构表示，便于用户控制 | 精确、可控、编辑友好 |
| **NeuralEditor** (Chen et al., 2023b) | 用K-D树构建点云引导的NeRF，编辑点位置并调整方向 | 点云易于操作，方向对 appearance 影响建模 | 保证方向-外观一致性，交互性强 |

#### 对象插入与擦除

| 方法 | 核心思路 | 为什么这么做 | 带来的好处 |
|------|-----------|---------------|--------------|
| **Neural scene graphs for dynamic scenes(Ost et al. (2021))** | 基于图结构，通过添加/删除叶节点操作对象 | 场景结构可扩展，可复用性强 | 插入/擦除对象简单直接 |
| **LaTeRF** (Mirzaei et al., 2022) | 回归“感兴趣对象”概率 + 用CLIP填补遮挡 | 语义感知 + 自适应补全 | 保证插入效果自然，遮挡处理鲁棒 |
| **Grf: Learning a general radiance field for 3d representation and rendering(Yang et al. (2021))** | 场景/对象分支+对象激活码库，控制对象运动与切换 | 支持对象重定位与动画插入 | 目标可动态控制，适合交互编辑 |
| **NeRFIn** (Liu et al., 2022a) | 利用用户绘制的 mask 引导 RGB-D 编辑 → NeRF更新 | 简单交互式擦除机制 | 用户友好，交互感强 |
| **SPIn-NeRF** (Mirzaei et al., 2023) | 用语义NeRF优化擦除掩码，实现一致性擦除 | 减少局部遮挡引起的伪影 | 消除冗余目标，保证局部/全局一致性 |
| **Removing objects from NeRFs(Weder et al. (2023))** | 引入 mask 引导的视图置信度，选择最优训练视角 | 提高擦除阶段重建稳定性 | 跨视图保持一致，训练更鲁棒 |
| **DiffRF** (Müller et al., 2023) | 基于扩散模型重建被mask区域，辅以体渲染损失 | 利用扩散机制生成新外观 | 完成度高、语义连续性强 |

#### 风格化编辑

| 方法 | 核心思路 | 为什么这么做 | 带来的好处 |
|------|-----------|---------------|--------------|
| **ClimateNeRF** (Li et al., 2023b) | 融合物理模拟 + instant-NGP，实现不同气候风格渲染 | 增强NeRF在复杂环境中的适应性 | 快速生成雾霾、洪水、降雪等数据 |
| **Upst-nerf: Universal photorealistic style transfer of neural radiance fields for 3d scene**Chen et al. (2024b) 等 | 多数聚焦于艺术风格迁移与特效渲染 | 降低风格化数据获取成本 | 构建多样化训练集，增强模型泛化 |
| **Neural lidar fields for novel view synthesis**Wang et al. (2023a)| 探索多模态驱动的风格场景合成 | 以语言/图像等模态控制风格 | 提高人机交互性，适配不同应用目标 |