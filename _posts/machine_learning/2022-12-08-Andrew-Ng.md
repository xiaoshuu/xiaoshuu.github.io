---
title: 吴恩达机器学习笔记
tags: 机器学习
sidebar:
  nav: cs
mathjax: true
key: 2022-12-08-Andrew-Ng
aside:
    toc: true
---

# 试图做些笔记

<!--more-->

```C
printf("Hello");
```

## What's Machine Learning
A computer program is said to learn from experience E with respect to some task T and some performance measure P,if its performance on T,as measured by P,improves with experience E.

## 一些符号（？）
![20221209222600](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209222600.png)<br />
$(x^i,y^i)$表示第i个数据集，即表格中第i行<br />
大写字母如A，B，X，Y表示矩阵，小写字母如a,b,x,y表示向量
![20221210000923](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210000923.png)<br />


## 监督学习

### 1.定义

#### 大致定义
给予机器一些正确答案（即数据集），让机器推测出更多的正确答案<br />
>eg.给不同平方的房子的价格，让机器人推测出任意面积大小房子的价格

#### 具体定义
1.Regression(回归问题)：<br />
Predict continuous valued output(price)<br />
设法预测连续值的属性<br />
2.Classification(分类问题)：<br />
Discrete valued output(in this example is 0 or 1)<br />
设法预测一个离散值输出<br />
![1](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/1.png)
<br />上图中仅有一个特征值，在实际应用中可能有多种特征值，forexample：
<br />![2](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/2.png)<br />
<center><p>图中⚪表示良性肿瘤，×表示恶性肿瘤</p></center>
当然还会有无穷多特征值的情况，等待后续学习。

### 2~7为连续型

### 2.单元线性回归模型
<br />![20221209223529](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209223529.png)<br />
图中h指的是函数；$\theta_{0}$与$\theta_{1}$为参数，类似y=kx+b中的k与b；uniavariable意思为单变量的
<br />![20221209224614](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209224614.png)<br />
目的是寻找使minimize后面那一串公式（懒得打了）的值最小的$\theta_{0}$与$\theta_{1}$，引入J($\theta_{0}$,$\theta_{1}$)，即代价函数，该种代价函数也被称为平方误差（代价）函数，为解决回归问题最常用的手段<br />

#### 1.平方误差代价函数相关

当h中$\theta_{1}=0$时得到的J($\theta_{0}$,$\theta_{1}$)与$\theta_{0}$的图像为抛物线，当$\theta_{1}\not=0$时得到的3D图像如下所示
<br />![20221209230836](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209230836.png)<br />
采用等高线（即在同一密闭曲线上的J值相等）得到的图像如下所示（地理中的盆地（雾））
<br />![20221209230807](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209230807.png)<br />

#### 2.梯度下降算法

问题描述
<br />![20221209231248](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209231248.png)<br />
直观表示就是随机从一个点出发，寻找周围梯度最小的方向，往前走一步，然后再寻找梯度最小的方向，不断重复以上过程
<br />![20221209231557](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209231557.png)<br />
如图中所示，如果从不同的点出发会到不同的局部最优解<br />
接下来是数学过程（没想到在这也能遇到微积分 sad）
<br />![20221209232551](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209232551.png)<br />
<center><p>右边为错误算法，两个参数应同时更新</p></center>

>图中:=为赋值运算符<br />
>如果说a:=b，则是将b的值赋给a<br />
>如果说a=b，则是判定a是否等于b，即a==b

>$\alpha$被称为学习率，永远是正值，用来控制梯度下降时迈出多大的步子，$\alpha$越大表示梯度下降的速度越快，$\alpha$过小则收敛速度过慢，$\alpha$过大则可能导致无法收敛甚至发散

有关为什么是减偏导数
<br />![20221209233614](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209233614.png)<br />

关于$\theta_{1}$到局部最优解时梯度下降法在干些啥
<br />![20221209233828](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209233828.png)<br />
解释了为啥即使选取了合适的$\alpha$梯度下降也可能收敛到局部最优而不是全局最优

<br />![20221209234113](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209234113.png)<br />

#### 3.混~合~~到一起~

<br />![20221209234545](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209234545.png)<br />
虽然可能有局部最优解问题的出现，但平方误差代价函数的图像是一个碗，所以不存在这个问题<br />
将平方误差代价函数用梯度下降来求最优解的方法是Batch算法的一种，其定义为：Each step of gradient descent uses all the training examples(每一步梯度下降都历遍了整个训练集的样本)，体现在该方法中为那个求和

### 3.多元线性回归模型

<br />![20221210001929](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210001929.png)<br />

<br />![20221210002048](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210002048.png)<br />
<center><p>图中划掉部分表示可以用一个小写字母，即向量来表示它们整体</p></center>
<br />![20221210002337](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210002337.png)<br />

#### 特征缩放

当不同特征值的取值范围相差很大时会导致代价函数的图象挤到一起，在采用梯度下降算法时会走很多崎岖的路，故此时需采用特征缩放
![20221210004903](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210004903.png)
![20221210005414](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210005414.png)
![20221210010027](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210010027.png)

### 4.保证梯度下降在正常工作

<br />![20221210135448](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210135448.png)<br />
画出代价函数随迭代步数增加的变化曲线来判断梯度下降算法是否已经收敛

### 5.学习率的选择

<br />![20221210135956](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210135956.png)
![20221210140354](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210140354.png)

### 6.线性回归模型中特征值的选择

![20221210141612](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210141612.png)<br />
当然这不是唯一的一种选择，在该例子中还可选择开方，可以选多种特征值组合最后看哪个方案得到的代价函数最小

### 7.正规方程（比梯度下降更直接的方法）

最小二乘法（救命这是啥啊全忘了sos）
<br />![20221210143820](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210143820.png)<br />
用正规方程法不用进行特征缩放
<br />![20221210144640](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210144640.png)<br />
如果$X^TX$不可逆如何解决（貌似以下过程在编译过程中会自动完成）<br />
<br />![20221210150004](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210150004.png)<br />

### 8.分类问题

![20221210151012](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210151012.png)<br />
显然线性回归模型现在不太适用，引入新的算法Logistic Regression Model

#### Logistic Regression Model

##### 1.对该模型的一些阐述
![20221210163724](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210163724.png)
![20221210165111](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210165111.png)
![20221210165801](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210165801.png)
![20221210171708](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210171708.png)
![20221210172052](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210172052.png)

##### 2.该模型使用的代价函数与取参数的多种算法
如果接着采用之前的平方误差代价函数会导致代价函数有许多局部最优解，所以更新换代了！换成如下的代价函数保证得到的是凸函数（弹幕里说这里凸函数和国内的定义是反的 但我根本不记得这玩意的定义是啥了）<br />
![20221210174938](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210174938.png)
![20221210175115](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210175115.png)<br />
下面对代价函数进行简化
![20221210175449](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210175449.png)
![20221210180441](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210180441.png)
![20221210183903](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210183903.png)

##### 3.多元分类问题：一对多
![20221210184059](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210184059.png)
![20221210184524](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210184524.png)
![20221210184907](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221210184907.png)

## 正则化相关

### 什么是过拟合和欠拟合

#### 线性回归中
![20221212163405](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212163405.png)

#### 逻辑回归中
![20221212163620](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212163620.png)

### 如何解决过拟合
![20221212163743](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212163743.png)

### 代价函数中进行正则化
![20221212164337](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212164337.png)
![20221212165154](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212165154.png)
![20221212165510](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212165510.png)

### 线性回归的正则化

梯度下降的
<br />![20221212170134](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212170134.png)<br />
正规化方程的
<br />![20221212170446](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212170446.png)<br />
![20221212170822](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212170822.png)

### Logistic回归的正则化

![20221212171136](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212171136.png)
![20221212171208](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212171208.png)<br />

#### 插播点东西

![20221212171518](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212171518.png)
![20221212171610](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212171610.png)
![20221212171650](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212171650.png)<br />
谢谢您 这就去退学去硅谷

## 非线性假设

### 神经网络

![20221212172454](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212172454.png)<br />

#### 神经元模型

![20221212173744](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212173744.png)

#### 神经网络模型
![20221212174552](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212174552.png)<br />

#### 神经网络中的计算

![20221212175827](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212175827.png)
![20221212181755](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212181755.png)<br />
这个神经网络所做的事就像是逻辑回归，但不是使用原先的$x_{1}，x_{2}，x_{3}$作为特征，而是用$a_{1}^{(2)}，a_{2}^{(2)}，a_{3}^{(2)}$作为新的特征值，这些是学习得到的函数输入值，具体来说，就是从第一层映射到第二层的函数，这个函数由其他参数$\theta^{(1)}$决定（相当于通过原始特征计算出更好的特征作为训练模型使用）

#### 有关神经网络如何计算复杂非线性函数的输入的例子

![20221212192528](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212192528.png)
![20221212192806](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212192806.png)
![20221212192852](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212192852.png)
![20221212193328](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212193328.png)

#### 神经网络实现多元分类

![20221212193809](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212193809.png)
![20221212193836](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221212193836.png)

#### 神经网络中的代价函数

![20221214222927](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214222927.png)
![20221214223634](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214223634.png)

#### 反向传播算法

![20221214224758](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214224758.png)
![20221214225555](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214225555.png)
![20221214230040](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214230040.png)

#### 试图理解反向传播

先复习一哈正向传播<br />
![20221214231139](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214231139.png)
![20221214231524](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214231524.png)
![20221214232221](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214232221.png)

#### 梯度检验

在反向传播的过程中很容易产生一些bug，但虽然有bug它的代价函数还是能不断减少，但最后得到的数据会比没有bug高一个量级，故需要一个东西来检验一下是否出现了bug，这就是（当当当当）梯度检验！（试图伪装出我热爱学习的样子）<br />
![20221214233500](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214233500.png)
![20221214233600](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214233600.png)
![20221214234147](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214234147.png)

#### 如何选取初始权重值

![20221214234524](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214234524.png)
![20221214234700](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221214234700.png)<br />
总结如何~~搞~~训练神经网络：<br />
首先将权重随机初始化为一个接近0的范围在$-\epsilon$到$\epsilon$之间的数，然后进行反向传播，再进行梯度检验，最后使用梯度下降或者其他高级优化算法来最小化代价函数J($\theta$)，整个过程从为参数选取一个随机初始化的值开始，随机初始化的选取过程是一个打破对称性的流程，随后通过梯度下降或者高级优化算法就能计算出$\theta$的最优值。

#### 最终的总结
![20221215002909](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215002909.png)
![20221215001854](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215001854.png)
![20221215003214](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215003214.png)
![20221215003501](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215003501.png)<br />
此处老师举了个无人驾驶的例子 1992年就实现了我焯 好牛

## 如何选择算法啥的

### 如何评估机器学习算法的性能

![20221215231420](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215231420.png)
![20221215231753](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215231753.png)
![20221215232321](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215232321.png)
![20221215232445](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215232445.png)
![20221215232747](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215232747.png)
![20221215233622](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215233622.png)
![20221215233927](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215233927.png)

### 偏差和方差和正则化的关系

![20221215235614](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215235614.png)
![20221216003420](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221216003420.png)
![20221216003926](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221216003926.png)

### 机器学习诊断法 

![20221215004348](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221215004348.png)
![20221216004343](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221216004343.png)
![20221216004728](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221216004728.png)
![20221216004935](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221216004935.png)
![20221216005620](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221216005620.png)
![20221216010113](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221216010113.png)<br />
又开始夸大家超过大部分的硅谷工程师了，老师您在夸的时候能收敛一下脸上的谜之笑容吗

## 机器学习系统设计

### 误差分析

1.可以手动分析数据集，将交叉数据集分类并查看分类出来的邮件是否符合所设的特征，如果match了很少的特征则可以将这个特征砍掉<br />
2.通过数值方法（numerical evaluation）来评估算法的效果<br />
总之就是在犹豫一种算法时不如直接在交叉验证集上用单一规则的数值评价指标，观察误差率是变大了还是变小了来决定搞不搞，莽就完了

### 如何使用一个合适的误差度量值（偏斜类问题）

![20221218003336](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221218003336.png)
![20221218004210](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221218004210.png)
![20221218004951](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221218004951.png)
![20221218005323](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221218005323.png)

### 什么时候用更多数据可以解决问题

1.特征值x有充足的信息来确定y，eg.让电脑做完形填空<br />
如何判定x是不是有充足的信息来确定y呢，可以假设让人类专家来答这个问题，一位英语专家显然能很好地做完形填空，故这种就是x有充足的信息来确定y；但一个金融专家显然不能很好地预测哪支股票哪支基金会涨（呜呜我的钱QAQ），这种就是x没有充足的信息来确定y<br />
2.用一个超级无敌复杂的模型时也可以用很多的数据<br />
省流：特征超多或模型超复杂时用更多的数据来喂饱这个模型吧！

## 支持向量机（SVM）


![20221218162129](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221218162129.png)
![20221218162514](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221218162514.png)
![20221218164100](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221218164100.png)<br />
>图中$A+\lambda B$与$CA+B$中的$\lambda$与$C$均为给B和A设权重，来决定我们是更关心第一项的优化还是更关心第二项的优化（C<1）<br />

![20221218164654](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221218164654.png)

### 为什么支持向量机又称大间隔分类器

![20221222202946](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222202946.png)
![20221222204229](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222204229.png)
![20221222204602](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222204602.png)

### 支持向量机的数学原理

![20221222205417](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222205417.png)
![20221222205949](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222205949.png)
![20221222210532](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222210532.png)

### 核函数

![20221222210822](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222210822.png)
![20221222211326](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222211326.png)
![20221222211537](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222211537.png)
![20221222212007](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222212007.png)
![20221222212326](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222212326.png)
![20221222222734](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222222734.png)
![20221222222944](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222222944.png)
![20221222223355](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222223355.png)
![20221222223507](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222223507.png)

### 使用SVM

![20221222224334](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222224334.png)
![20221222224922](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221222224922.png)
![20221223000359](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221223000359.png)
![20221223000529](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221223000529.png)
![20221223000607](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221223000607.png)

## 无监督学习

### 大致定义
给予机器没有标准答案的数据让机器自己分类找出数据的结构，eg.聚类算法，鸡尾酒会算法
<br />![20221209221933](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221209221933.png)<br />
<center><p>据说这样一行就可以了 amazing</p></center>
好吧 是在octave里<br />

### K-Means算法
![20221228205036](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221228205036.png)
![20221228205202](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221228205202.png)
![20221228210016](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221228210016.png)

#### 代价函数 

![20221228210511](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221228210511.png)
![20221228210720](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221228210720.png)
![20221228222648](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221228222648.png)

#### 如何选择K值（聚类数量）

![20221229060435](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229060435.png)
![20221229060926](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229060926.png)

### 降维(PCA)

删除高度冗余的特征<br/>
![20221229061927](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229061927.png)
![20221229062122](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229062122.png)
![20221229065900](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229065900.png)
![20221229072404](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229072404.png)
![20221229073142](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229073142.png)
![20221229074129](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229074129.png)
![20221229075044](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229075044.png)
![20221229075225](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229075225.png)
![20221229075353](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229075353.png)

#### 如何将PCA后得到的低维数据还原成高维数据

![20221229101826](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229101826.png)

#### 如何选择k

![20221229102925](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229102925.png)
![20221229103218](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229103218.png)

#### 应用PCA的建议

![20221229104314](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229104314.png)
![20221229104807](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229104807.png)
![20221229105330](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229105330.png)

### 异常检测

![20221229105848](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229105848.png)
![20221229111424](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229111424.png)
![20221229111927](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229111927.png)
![20221229113807](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229113807.png)
![20221229113956](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229113956.png)
![20221229114211](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229114211.png)
![20221229195017](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229195017.png)
![20221229195502](https://cdn.jsdelivr.net/gh/xiaoshuu/img/Picgo/20221229195502.png)